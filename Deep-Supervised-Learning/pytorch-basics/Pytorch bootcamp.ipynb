{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c1eb7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c9228a",
   "metadata": {},
   "source": [
    "# Pytorch Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ce5b9a",
   "metadata": {},
   "source": [
    "Why do we use tensors? Because they have gradients, so when you perform any operation with the tensor, pytorch does the math and stores in the tensor what it has done and how to backpropagate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be14034d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27])\n",
      "torch.Size([28])\n"
     ]
    }
   ],
   "source": [
    "# 1D Tensor (28)\n",
    "y = torch.tensor([i for i in range(28)])\n",
    "print(y)\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1ac70b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28])\n"
     ]
    }
   ],
   "source": [
    "# 2D Tensor 28x28 (for example an image of 28 by 28 pixels)\n",
    "x = torch.tensor([[i for i in range(28)] for _ in range(28)])\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0b5259a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784])\n",
      "torch.Size([1, 784])\n"
     ]
    }
   ],
   "source": [
    "# We can reshape the tensor with the view function\n",
    "# A good way of using view is using -1 as its first arg, because this flattens out the tensor\n",
    "\n",
    "x1 = x.view(-1) # Will go from 2D to 1D\n",
    "x2 = x.view(-1,784) # Will stay in 2D\n",
    "\n",
    "print(x1.size())\n",
    "print(x2.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6b7ea57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n",
      "tensor(5)\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Tensor Calculations\n",
    "\n",
    "a = torch.tensor([0,1])\n",
    "b = torch.tensor([1,0])\n",
    "\n",
    "print(a @ b)\n",
    "\n",
    "c = torch.tensor([[1,2,3], [4,5,6]])\n",
    "\n",
    "print(c[1][1])\n",
    "\n",
    "# Prints out a tensor, in order to get just the actual value:\n",
    "\n",
    "print(c[1][1].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac95ad3a",
   "metadata": {},
   "source": [
    "# Simple datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316a6263",
   "metadata": {},
   "source": [
    "With almost any form of data worth feeding a neural network, preprocessing is essential. PyTorch handles this with an object it calls the \"Dataset\". Later in this tutorial, we will see how to build our own dataset. For now, we'll use one of their provided datasets so we can examine the objects torch provides.\n",
    "\n",
    "Tensors go into datasets via the \"Dataset\" object (take care of the preprocessing) and then we feed this dataset into another object called the \"Dataloader\". So we have to import two essential things to make this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97352d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\r\n",
      "      - Validating: \u001b[32mOK\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader \n",
    "from torchvision.transforms import ToTensor\n",
    "import numpy as np \n",
    "\n",
    "import tqdm\n",
    "from tqdm.notebook import trange\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "\n",
    "from PIL import Image #To see images externally\n",
    "import matplotlib.pyplot as plt #To see images in notebook\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63800f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fashion MNIST dataset\n",
    "# The root argument is to let the function know where we want to store the data\n",
    "\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root = \"data\",\n",
    "    train = True,\n",
    "    download = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "739129e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=L size=28x28 at 0x7FB0FB175B20>, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will say which image and which label\n",
    "\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2ac7195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To see how the image looks like\n",
    "\n",
    "img = train_data[0][0]\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94bb45e",
   "metadata": {},
   "source": [
    "Training data in its raw form is an image object (not so useful because we can't feed it in a NN) and this is why we transform it to a Tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6250a209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trans = ToTensor()\n",
    "img = trans(train_data[0][0])\n",
    "plt.imshow(img.squeeze(), cmap='gray') # In this case we have to use squeeze because tensor is (1x28x28) (removes 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1863a855",
   "metadata": {},
   "source": [
    "Luckily we can do this from the beginning when loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0c6dcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data length: 60000\n",
      "test data length: 60000\n"
     ]
    }
   ],
   "source": [
    "train_data = datasets.FashionMNIST( \n",
    "    root = \"data\", \n",
    "    train = True, \n",
    "    download = True, \n",
    "    transform = ToTensor() \n",
    ") \n",
    "test_data = datasets.FashionMNIST( \n",
    "    root = \"data\", \n",
    "    train = False, \n",
    "    download = True, \n",
    "    transform = ToTensor() \n",
    ") \n",
    "\n",
    "print(\"training data length:\", len(train_data)) \n",
    "print(\"test data length:\", len(train_data)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e67e3174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = train_data[0][0]\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35bb6fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rand = np.random.randint(len(train_data)) \n",
    "plt.imshow(train_data[rand][0].squeeze(), cmap='gray') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88aeca4",
   "metadata": {},
   "source": [
    "We also want to feed in the images in batches, and that is where the dataloader comes in. The dataloader is made to be iterated, so we have to build a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11987eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size = 16, shuffle=True) \n",
    "test_loader = DataLoader(test_data, batch_size = len(train_data), shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "880edbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape is  torch.Size([16, 1, 28, 28])\n",
      "Labels shape is  torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "for x, label in train_loader: \n",
    "    print(\"Image shape is \",x.shape) \n",
    "    print(\"Labels shape is \",label.shape) \n",
    "    break "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e644a053",
   "metadata": {},
   "source": [
    "Another problem is that we can't feed an image to a NN (because it is 28x28 tensor) so we have to make a 1D tensor, so don't forget this! The above tells us that we have to resize the images once more, to flatten them into single-dimensional tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a16031e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape is  torch.Size([16, 784])\n",
      "Labels shape is  torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "for x, label in train_loader:\n",
    "    x = x.reshape(-1,28*28)\n",
    "    print(\"Image shape is \",x.shape)\n",
    "    print(\"Labels shape is \",label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f4217c",
   "metadata": {},
   "source": [
    "# Simple Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3c33216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two Ways of doing this, both of them do the exact same thing\n",
    "# Only a syntax difference, so your choice\n",
    "\n",
    "# We can make NNs look like regular python classes\n",
    "\n",
    "### FIRST OPTION #######\n",
    "\n",
    "class BasicNeuralNet1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicNeuralNet1, self).__init__()\n",
    "        self.layer1 = nn.Linear(784, 256)\n",
    "        self.layer2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784) # depends on whether you cleaned x before or not\n",
    "        x = torch.sigmoid(self.layer1(x)) # Activation function\n",
    "        x = self.layer2(x)\n",
    "        return F.log_softmax(x, 0)\n",
    "    \n",
    "    \n",
    "### SECOND OPTION #######\n",
    "### THIS COULD BE HANDY FOR AN AUTOENCODER #####\n",
    "\n",
    "class BasicNeuralNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicNeuralNet2, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(784, 256),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(256, 10),\n",
    "            nn.LogSoftmax(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = self.layer1(x)\n",
    "        return self.layer2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc16a9a",
   "metadata": {},
   "source": [
    "# Train that Model! (The two key pieces)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85759da",
   "metadata": {},
   "source": [
    "The two fundamental things here are the loss function and the optimizer (how it should improve). PyTorch has an extensive library with many different loss functions and optimizers.\n",
    "\n",
    "It's traditional to have a big list of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3684ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicNeuralNet1()\n",
    "learning_rate = 0.05\n",
    "epochs = 10\n",
    "\n",
    "MSELoss = torch.nn.MSELoss()\n",
    "CELoss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e269ec",
   "metadata": {},
   "source": [
    "It's also good to define a test/accuracy function, to keep tabs on how the model is doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c9f97ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(output, targets): \n",
    "    \"\"\"\n",
    "    calculates accuracy from model output and targets \n",
    "    \n",
    "    \"\"\" \n",
    "    output = output.detach() # Detach the predictions from the models to get rid of the gradients\n",
    "    predicted = output.argmax(-1) \n",
    "    correct = (predicted == targets).sum().item() # Sum up the number of correct values\n",
    "    accuracy = correct / output.size(0) * 100 \n",
    "    return accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc1a48d",
   "metadata": {},
   "source": [
    "Use the trange-function from tqdm.notebook, because it is a very handy function for visualizing progress. It acts just like the normal python \"range\", except it displays a progress bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "adea92db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train Acc 93.75% | Test Acc 75.56\n",
      "Epoch 1. Train Acc 68.75% | Test Acc 77.66999999999999\n",
      "Epoch 2. Train Acc 68.75% | Test Acc 77.64\n",
      "Epoch 3. Train Acc 62.5% | Test Acc 76.06\n",
      "Epoch 4. Train Acc 68.75% | Test Acc 77.98\n",
      "Epoch 5. Train Acc 87.5% | Test Acc 78.36999999999999\n",
      "Epoch 6. Train Acc 87.5% | Test Acc 79.94\n",
      "Epoch 7. Train Acc 87.5% | Test Acc 78.91\n",
      "Epoch 8. Train Acc 81.25% | Test Acc 76.83\n",
      "Epoch 9. Train Acc 75.0% | Test Acc 78.39\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs): \n",
    "    \n",
    "    # Very important to put this in the beginning, \n",
    "    # because we want the gradients to be available\n",
    "    model.train()\n",
    "    \n",
    "    for data, labels in train_loader: \n",
    "        # Clear out the gradients before doing the forward and backward pass \n",
    "        # because we have to zero out the gradients from previous training\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # run data through model \n",
    "        preds = model(data) \n",
    "        \n",
    "        # compare predictions to actual labels \n",
    "        loss = CELoss(preds,labels) \n",
    "        \n",
    "        # After we have found the predictions and found the loss\n",
    "        # we have to backpropogate the loss \n",
    "        loss.backward() \n",
    "        \n",
    "        # Adjust the model parameters via the optimizer\n",
    "        # To update the parameters\n",
    "        optimizer.step() \n",
    "        \n",
    "    # After each epoch, it's nice to see how our model is doing \n",
    "    model.eval() # disables gradient computations during testing + speeds things up\n",
    "    test_data, test_labels = next(iter(test_loader)) \n",
    "    test_data = test_data.reshape(-1,784) \n",
    "    test_preds = model(test_data) \n",
    "    test_acc = get_accuracy(test_preds,test_labels) \n",
    "    train_acc = get_accuracy(preds, labels) \n",
    "    print(f\"Epoch {epoch}. Train Acc {train_acc}% | Test Acc {test_acc}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
